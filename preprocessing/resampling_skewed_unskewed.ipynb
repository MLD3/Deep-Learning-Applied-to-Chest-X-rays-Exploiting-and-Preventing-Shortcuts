{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsampling data for 'Skewed' and 'Unskewed' training sets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## search for \"TODOs\" to complete the file to run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from scipy.stats import spearmanr\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "# helper function that checks to make sure there is no patient overlap in training, validation, and test sets \n",
    "# takes in metadata where patients are defined by a unique \"pt_id\" column, splits is a list of data splits (E.g., \"triain\", \"valid\", \"test\") in the \"split\" column \n",
    "def check_pt_overlap(meta, splits):\n",
    "    pts_all =[]\n",
    "    for split in splits:\n",
    "        pts = meta[\"pt_id\"][meta[\"split\"] == split].unique()\n",
    "        pts_all.append(pts)\n",
    "    for x, y in combinations(pts_all, 2):\n",
    "        try:\n",
    "            assert(not np.intersect1d(x,y))\n",
    "        except:\n",
    "            return 0 \n",
    "    return 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File  does not exist: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-526199ed190e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO: define path to metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmeta_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# TODO: define bias attribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File  does not exist: ''"
     ]
    }
   ],
   "source": [
    "# TODO: define path to metadata \n",
    "meta_file = ''\n",
    "meta = pd.read_csv(meta_file)\n",
    "\n",
    "# TODO: define bias attribute \n",
    "bias = ''\n",
    "\n",
    "#TODO: define disease column name you want to skew against (e.g., Pneumonia in the MIMIC-CXR or CheXpert datasets)\n",
    "disease = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly sample test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose n_test patients at random for test set s.t. there is no correlation between the disease and the bias \n",
    "n_test = 1000\n",
    "np.random.seed(1)\n",
    "seed = 0\n",
    "correl_1 = 1\n",
    "# keep resampling a test set until there is no correlation between disease and bias \n",
    "while np.abs(correl_1) > 0.03:\n",
    "    np.random.seed(seed)\n",
    "    seed += 1\n",
    "    test_pt_ids = np.random.choice(meta.orig_pt_id.unique(), n_test, replace = False)\n",
    "    test_pts =  meta[meta.orig_pt_id.isin(test_pt_ids)]\n",
    "    \n",
    "    # this is the subset of patients left for training and validation   \n",
    "    meta_left = meta[~meta.orig_pt_id.isin(test_pt_ids)]        \n",
    "    \n",
    "    # NOTE/TODO: in the paper, we sampled the test set s.t. there was no correlation between the disease and \n",
    "    # ANY of the biased attributes, so you should copy and past the line below to track all correlations \n",
    "    correl_1 = spearmanr(test_pts[disease], test_pts[bias])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpearmanrResult(correlation=0.020925235899468216, pvalue=0.41218490933870977)\n",
      "SpearmanrResult(correlation=0.0040481585583360446, pvalue=0.8739607104499522)\n"
     ]
    }
   ],
   "source": [
    "# look at correlation\n",
    "print(spearmanr(test_pts[disease], test_pts[bias]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample training sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meta_left' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-510f5e7166d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# subset of patients with the same disease label as the bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msame_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_left\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmeta_left\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdisease\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmeta_left\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msame_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_left\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmeta_left\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdisease\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmeta_left\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'meta_left' is not defined"
     ]
    }
   ],
   "source": [
    "# subset of patients with the same disease label as the bias \n",
    "same_pos = meta_left[meta_left[disease] == 1][meta_left[bias] == 1]\n",
    "same_neg = meta_left[meta_left[disease] == 0][meta_left[bias] == 0]\n",
    "\n",
    "# subset of patients with the opposite disease label as the bias \n",
    "opp_pos = meta_left[meta_left[disease] == 0][meta_left[bias] ==1]\n",
    "opp_neg = meta_left[meta_left[disease] == 1][meta_left[bias] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampling so that unskewed and skewed training sets end up being the same size \n",
    "if len(same_pos) < len(opp_pos):\n",
    "    same = pd.concat([same_pos, opp_pos.sample(len(same_pos))])\n",
    "else:\n",
    "    same = pd.concat([opp_pos, same_pos.sample(len(opp_pos))])\n",
    "    \n",
    "if len(same_neg) < len(opp_neg):\n",
    "    opp = pd.concat([same_neg, opp_neg.sample(len(same_neg))])\n",
    "else:\n",
    "    opp = pd.concat([opp_neg, same_neg.sample(len(opp_neg))])\n",
    "    \n",
    "no_correl = pd.concat([same, opp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpearmanrResult(correlation=0.9999999999999999, pvalue=0.0)\n",
      "SpearmanrResult(correlation=-0.01106187632232126, pvalue=0.28841820019134723)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'correl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-12956b0e7e8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# correl_0 = correl_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'correl' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "# skewed training set \n",
    "\n",
    "correl_1_1 = pd.concat([same_pos, same_neg])\n",
    "print(spearmanr(correl_1_1[\"Pneumonia\"], correl_1_1[bias]))\n",
    "\n",
    "# unskewed training set\n",
    "correl_0 = pd.concat([no_correl])\n",
    "\n",
    "# sampling here so that skewed and unskewed are the same size \n",
    "if len(correl_1_1) > len(correl_0):\n",
    "    correl_1_1 = correl_1_1.sample(len(correl_0))\n",
    "else:\n",
    "    correl_0 = correl_0.sample(len(correl_1_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick train and validation for each - 1:1 correlation \n",
    "\n",
    "np.random.seed(2)\n",
    "valid_pt_ids_1_1 = np.random.choice(correl_1_1.orig_pt_id.unique(), 1000, replace = False)\n",
    "valid_pts_1_1 =  correl_1_1[correl_1_1.orig_pt_id.isin(valid_pt_ids_1_1)]\n",
    "train_pts_1_1 = correl_1_1[~correl_1_1.orig_pt_id.isin(valid_pt_ids_1_1)]\n",
    "\n",
    "test_pts[\"split\"] = \"test\"\n",
    "valid_pts_1_1[\"split\"] = \"valid\"\n",
    "train_pts_1_1[\"split\"] = \"train\"\n",
    "\n",
    "meta_1_1 = pd.concat([test_pts, valid_pts_1_1, train_pts_1_1])\n",
    "\n",
    "\n",
    "# look at correlation between train and valid \n",
    "print(spearmanr(train_pts_1_1[\"Pneumonia\"], train_pts_1_1[bias]))\n",
    "print(spearmanr(valid_pts_1_1[\"Pneumonia\"], valid_pts_1_1[bias]))\n",
    "print(spearmanr(test_pts[\"Pneumonia\"], test_pts[bias]))\n",
    "\n",
    "assert(check_pt_overlap(meta_1_1, (\"train\", \"valid\", \"test\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpearmanrResult(correlation=0.9999999999999998, pvalue=0.0)\n",
      "SpearmanrResult(correlation=1.0, pvalue=0.0)\n",
      "SpearmanrResult(correlation=0.020925235899468216, pvalue=0.41218490933870977)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/arcts/centos7/anaconda3/5.2/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/sw/arcts/centos7/anaconda3/5.2/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "/sw/arcts/centos7/anaconda3/5.2/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/sw/arcts/centos7/anaconda3/5.2/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# pick train and validation for each - skewed \n",
    "np.random.seed(2)\n",
    "n_valid = 1000 \n",
    "valid_pt_ids_1_1 = np.random.choice(correl_1_1.orig_pt_id.unique(), n_valid, replace = False)\n",
    "valid_pts_1_1 =  correl_1_1[correl_1_1.orig_pt_id.isin(valid_pt_ids_1_1)]\n",
    "train_pts_1_1 = correl_1_1[~correl_1_1.orig_pt_id.isin(valid_pt_ids_1_1)]\n",
    "\n",
    "test_pts[\"split\"] = \"test\"\n",
    "valid_pts_1_1[\"split\"] = \"valid\"\n",
    "train_pts_1_1[\"split\"] = \"train\"\n",
    "\n",
    "meta_1_1 = pd.concat([test_pts, valid_pts_1_1, train_pts_1_1])\n",
    "\n",
    "\n",
    "# look at correlation between train and valid \n",
    "print(spearmanr(train_pts_1_1[disease], train_pts_1_1[bias]))\n",
    "print(spearmanr(valid_pts_1_1[disease], valid_pts_1_1[bias]))\n",
    "print(spearmanr(test_pts[disease], test_pts[bias]))\n",
    "assert(check_pt_overlap(meta_1_1, (\"train\", \"valid\", \"test\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpearmanrResult(correlation=-0.01423142355941618, pvalue=0.2052050990211948)\n",
      "SpearmanrResult(correlation=0.00891895436390272, pvalue=0.7493196852805653)\n",
      "SpearmanrResult(correlation=0.020925235899468216, pvalue=0.41218490933870977)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/arcts/centos7/anaconda3/5.2/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/sw/arcts/centos7/anaconda3/5.2/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "/sw/arcts/centos7/anaconda3/5.2/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/sw/arcts/centos7/anaconda3/5.2/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# pick train and validation for each - unskewed\n",
    "\n",
    "np.random.seed(2)\n",
    "n_valid = 1000 \n",
    "valid_pt_ids_0 = np.random.choice(correl_0.orig_pt_id.unique(), n_valid, replace = False)\n",
    "valid_pts_0 =  correl_0[correl_0.orig_pt_id.isin(valid_pt_ids_0)]\n",
    "train_pts_0 = correl_0[~correl_0.orig_pt_id.isin(valid_pt_ids_0)]\n",
    "\n",
    "test_pts[\"split\"] = \"test\"\n",
    "valid_pts_0[\"split\"] = \"valid\"\n",
    "train_pts_0[\"split\"] = \"train\"\n",
    "\n",
    "meta_0 = pd.concat([test_pts, valid_pts_0, train_pts_0]).sample(frac = 1)\n",
    "\n",
    "\n",
    "# look at correlation between train and valid \n",
    "print(spearmanr(train_pts_0[disease], train_pts_0[bias]))\n",
    "print(spearmanr(valid_pts_0[disease], valid_pts_0[bias]))\n",
    "print(spearmanr(test_pts[disease], test_pts[bias]))\n",
    "\n",
    "assert(check_pt_overlap(meta_0, (\"train\", \"valid\", \"test\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save both to csv files \n",
    "meta_1_1.to_csv(\"skewed.csv\")\n",
    "meta_0.to_csv(\"unskewed.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
