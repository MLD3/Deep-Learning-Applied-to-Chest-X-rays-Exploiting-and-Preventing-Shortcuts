{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File to get AUROC for a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from evaluate_models import *\n",
    "import os\n",
    "import torch\n",
    "from utils import make_auc_plot, make_precision_recall\n",
    "from utils import config, plot_auc\n",
    "from matplotlib import pyplot as plt\n",
    "from utils import get_roc_CI\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0,1'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to plot AUROC \n",
    "# takes list of ground truth values, predictions, and labels for the plot \n",
    "def plot_auc(y_true, y_pred, labels):\n",
    "    plt.tight_layout()\n",
    "    colors = ['r','b','g', 'c', 'm']\n",
    "    lw = 2\n",
    "    fpr = []\n",
    "    tpr = []\n",
    "    auc = []\n",
    "    n_classes = len(labels)\n",
    "        \n",
    "    if(n_classes == 1):\n",
    "        color = colors[0]\n",
    "        fpr, tpr, _ = metrics.roc_curve(y_true, y_pred)\n",
    "\n",
    "        \n",
    "        auc = metrics.roc_auc_score(y_true, y_pred)\n",
    "        \n",
    "        roc_curves, auc_scores, mean_fpr, tprs_lower, tprs_upper = get_roc_CI(y_true, y_pred)\n",
    "        plt.fill_between(mean_fpr, tprs_lower, tprs_upper, alpha=.1, color=color)\n",
    "        conf_int = ' ({:.2f}-{:.2f})'.format(np.percentile(auc_scores, 2.5), np.percentile(auc_scores, 97.5))\n",
    "        test = 'ROC curve of ' + labels[0] + ' area = {1:0.2f}'.format(0, auc) + conf_int\n",
    "        print(test)\n",
    "        plt.plot(fpr, tpr, color=color, lw=lw,\n",
    "                 label=test)            \n",
    "    else:\n",
    "        for ind in range(len(labels)):\n",
    "            f, t, _ = metrics.roc_curve(np.squeeze(y_true)[:,ind], np.squeeze(y_pred)[:,ind])\n",
    "            fpr.append(f)\n",
    "            tpr.append(t) \n",
    "            auc.append(metrics.roc_auc_score(y_true[:,ind], y_pred[:,ind]))\n",
    "    \n",
    "        for i, color in zip(range(len(labels)), colors):\n",
    "            roc_curves, auc_scores, mean_fpr, tprs_lower, tprs_upper = get_roc_CI(np.squeeze(y_true)[:,i], np.squeeze(y_pred)[:,i])\n",
    "\n",
    "            plt.fill_between(mean_fpr, tprs_lower, tprs_upper, alpha=.1, color=color)\n",
    "            conf_int = ' ({:.2f}-{:.2f})'.format(np.percentile(auc_scores, 2.5), np.percentile(auc_scores, 97.5))\n",
    "            if i == 0:\n",
    "                test = 'ROC curve of ' + labels[i] + ' area = {1:0.2f}'.format(i, auc[i]) + conf_int\n",
    "            elif i == 1:\n",
    "                test = 'ROC curve of ' + labels[i] + ' area = {1:0.2f}'.format(i, auc[i]) + conf_int\n",
    "            else:\n",
    "                test = 'ROC curve of ' + labels[i] + ' area = {1:0.2f}'.format(i, auc[i]) + conf_int\n",
    "\n",
    "#             else:\n",
    "#                 test = 'ROC curve of ' + labels[i] + ' area = {1:0.2f}'.format(i, auc[i])\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                 label=test)\n",
    "            print(test)\n",
    "    plt.axis('scaled')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right', fontsize = 'small')\n",
    "    plt.show()\n",
    "    return auc, np.percentile(auc_scores, 2.5), np.percentile(auc_scores, 97.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run test set through best models on all seeds + make plots accordingly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO: define model type and model name for config file \n",
    "model_type = \"\"\n",
    "model_name = \"\"\n",
    "\n",
    "# TODO: define path to log file \n",
    "df_search = pd.read_csv(\"\").drop_duplicates(subset = ['savename'], keep = \"first\")\n",
    "\n",
    "best_model_info,_ = get_best_model_across_seeds(df_search)\n",
    "predictions = []\n",
    "te_loader = []\n",
    "valid_scores = []\n",
    "pt_ids_all = []\n",
    "\n",
    "for model_info in best_model_info:\n",
    "    valid_scores.append(model_info.average_score)\n",
    "    config_str = model_type + \".\" + model_name \n",
    "    labels = config(config_str + \".labels\").split(\"|\")\n",
    "    checkpoint, model, criterion, exp = load_best_model(model_info, device, config_str, model_type, model_name)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    exp._get_data_loaders(model_info['seed'], checkpoint['params'], split = \"test\")\n",
    "    # test loader is always the last loader \n",
    "    te_loader = exp.loaders[-1]\n",
    "\n",
    "    y_true, y_pred, pt_ids = get_test_predictions(model, criterion, device, te_loader, model_name = model_name, get_all_predictions = True)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    pt_ids_all.append(np.unique(pt_ids))\n",
    "\n",
    "    score, unique_predictions, unique_truth, pt_ids = calc_roc(y_true, y_pred, pt_ids)\n",
    "    labels = config(config_str + \".labels\").split(\"|\")\n",
    "    plot_auc(unique_truth, unique_predictions, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Significance Tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical significance test takes in ground truth and predictions for two models, list of label values, and an alpha value \n",
    "def statistical_significance(y_true_first, y_pred_first, y_true_second, y_pred_second, labels, alpha = 0.05):\n",
    "    \n",
    "    n_classes = len(labels)\n",
    "    if (n_classes == 1):\n",
    "        _, auc_scores_first, _, _, _ = get_roc_CI(y_true_first, y_pred_first)\n",
    "        _, auc_scores_second, _, _, _ = get_roc_CI(y_true_second, y_pred_second)\n",
    "        difference = list((auc_scores_first[j] > auc_scores_second[j] for j in range(len(auc_scores_second))))\n",
    "        total_greater = sum(difference)\n",
    "        if total_greater / 1000 > 1 - alpha:\n",
    "            print(labels[0], \"statistically significant at p-value\", 1 - total_greater / 1000)\n",
    "        else:\n",
    "            print(labels[0], \"not statistically significant at p-value\", 1- total_greater / 1000)\n",
    "            \n",
    "    else:\n",
    "        for i in range(len(labels)):\n",
    "            #  get AUROC for 1000 bootstrapped samples \n",
    "            _, auc_scores_first, _, _, _ = get_roc_CI(np.squeeze(y_true_first)[:,i], np.squeeze(y_pred_first)[:,i])\n",
    "            _, auc_scores_second, _, _, _ = get_roc_CI(np.squeeze(y_true_second)[:,i], np.squeeze(y_pred_second)[:,i])\n",
    "            difference = list((auc_scores_first[j] > auc_scores_second[j] for j in range(len(auc_scores_second))))\n",
    "            total_greater = sum(difference)\n",
    "            print(total_greater)\n",
    "            if total_greater / 1000 > 1 - alpha:\n",
    "                print(labels[i], \"statistically significant at p-value\", 1 - total_greater / 1000)\n",
    "            else:\n",
    "                print(labels[i], \"not statistically significant at p-value\", 1 - total_greater / 1000)\n",
    "         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
