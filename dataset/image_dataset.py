from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
import os
import numpy as np
import pandas as pd
import torch
from matplotlib import pyplot as plt
from torch.utils.data import Dataset, DataLoader
from utils import config
from skimage import io
from PIL import Image
from dataset.image_utils import RandomCrop, RandomFlip, RandomRotate, ImageStandardizer
# import image_utils
from torchvision import transforms, utils
from importlib import reload
import pickle
from tqdm import tqdm
import cv2

def get_train_val_test_loaders(seed, config_str, batch_size, augmentation = [], num_classes = 3, data = "", labels = []):
    print("Get train_val_test_loaders")
    loaders, std = get_train_val_dataset(seed, config_str, augmentation, num_classes=num_classes, data = data, labels = labels)
    
    num_workers = 20
    def _init_fn(worker_id):
        np.random.seed(seed)
    
    for idx, loader in enumerate(loaders):
        loader = DataLoader(loader, batch_size = batch_size, shuffle = idx == 0, num_workers = num_workers, pin_memory = True, worker_init_fn = _init_fn)
        loaders[idx] = loader

    return loaders, std

def get_train_val_dataset(seed, config_str, augmentation, num_classes=3, data = "", labels = []):
    disk = config(config_str + ".disk")
    try:
        standardize = config(config_str + ".standardize")
    except:
        standardize = False
    loaders = []
    
    loaders.append(PacsDataset(seed, config_str, 'train', augmentation, num_classes, data, labels))
    loaders.append(PacsDataset(seed, config_str, 'valid', augmentation, num_classes, data, labels))
    loaders.append(PacsDataset(seed, config_str, 'test', augmentation, num_classes, data, labels))

    if (not disk):
        if (standardize == "mimic_chexpert"):
            print("Standardizing to mimic and chexpert dataset")
            image_mean = pickle.load(open("image_mean_no_border.p", "rb"))
            image_std = pickle.load(open("image_std_no_border.p", "rb"))
            standardizer = ImageStandardizer(image_mean, image_std)
        elif (standardize == "mimic_chexpert_gaussian"):
            print("Standardizing to mimic and chexpert gaussian dataset")
            mean_path = config(config_str + ".mean")
            std_path = config(config_str + ".std")
            image_mean = pickle.load(open(mean_path, "rb"))
            image_std = pickle.load(open(std_path, "rb"))
            
            standardizer = ImageStandardizer(image_mean, image_std)
        else:
            standardizer = ImageStandardizer()
            standardizer.fit(loaders[0].X)
        for idx, loader in enumerate(loaders):
            loader.X = standardizer.transform(loader.X)
            loaders[idx] = loader
    else:        
        if (standardize == "mimic_chexpert_gaussian"):
            print("Standardizing to mimic and chexpert gaussian dataset")
            mean_path = config(config_str + ".mean")
            std_path = config(config_str + ".std")
            image_mean = pickle.load(open(mean_path, "rb"))
            image_std = pickle.load(open(std_path, "rb"))
            
            standardizer = ImageStandardizer(image_mean, image_std)


        else:

            image_mean = pickle.load(open("image_mean_no_border.p", "rb"))
            image_std = pickle.load(open("image_std_no_border.p", "rb"))
            standardizer = ImageStandardizer(image_mean, image_std)
            print("standardizing:", image_mean, image_std)

        for idx, loader in enumerate(loaders):
            loader.standardizer = standardizer
            loaders[idx] = loader

    return loaders, standardizer



class PacsDataset(Dataset):

    def __init__(self, seed, config_str, partition, augmentation, num_classes=3, metadata = "", labels = []):
        """
        Reads in the necessary data from disk.
        """
        super().__init__()

        if partition not in ['train', 'valid', 'test']:
            raise ValueError('Partition {} does not exist'.format(partition))
        
        """
        Inititalize variables that tell me what model we're using
        """
        self.labels = labels
        np.random.seed(0)
        self.get_from_disk = config(config_str + ".disk")
        self.split = partition
        self.config_str = config_str
        self.image_size = int(config(self.config_str + ".image_size"))
        self.num_classes = config(self.config_str + ".num_classes")
        if (metadata == ""):
            self.metadata = pd.read_csv(config(self.config_str + '.csv_file'), index_col=0)
        else:
            self.metadata = pd.read_csv(metadata)
        self.class_labels = config(config_str + ".labels").split("|")
        if len(labels):
            for l in labels:
                self.class_labels.append(l)
                
        

            
        """
        Initialize augmentation variables 
        """
        self.augmentation = augmentation 
        self.list_of_transformations = []
        self.position = "uniform" if self.split == "train" else "center"
        self.crop = RandomCrop(seed, self.image_size, self.position) 
        
        if self.split == "train":
            if "flip" in self.augmentation:
                self.flip = RandomFlip(seed, config(self.config_str + ".flip_probability"))
                self.list_of_transformations.append(self.flip)

            if "rotate" in self.augmentation:
                self.rotate = RandomRotate(seed, config(self.config_str + ".rotate_degrees"))
                self.list_of_transformations.append(self.rotate)
                
        self.list_of_transformations.append(self.crop)
        self.composed = transforms.Compose(self.list_of_transformations)

        """
        Reads in the necessary data from disk.
        """
#         self.class_meta, self.X, self.y, self.y_orig = self._load_data()
        self.class_meta, self.X, self.y = self._load_data()

    def __len__(self):
        if (not self.get_from_disk):
            return len(self.X)
        else:
            return len(self.class_meta)
    
    def __getitem__(self, idx):
        if (self.get_from_disk):
            curr_data = self.class_meta.iloc[idx]
            
            img = self.composed(self.standardizer._transform_image(io.imread(curr_data["local_path"]/ 255).transpose(2,0,1)))

            y_lab = np.array([curr_data[l] for l in self.class_labels])
            pt_id = curr_data["pt_id"]

        else:
            img = np.squeeze(self.X[idx])
            img =  self.composed(img).transpose(2,0,1)
            
            y_lab = self.y[idx]


            pt_id = self.class_meta[idx]["pt_id"]
            
        return torch.from_numpy(img).float(), torch.tensor(y_lab).float(),torch.tensor(pt_id)

    
    def _load_data(self):
        """
        Loads a single data partition from file.
        """
#         print("loading %s..." % self.split)
        
        df = self.metadata[self.metadata.split == self.split]
        print("Getting from disk:", self.get_from_disk)
        if (self.get_from_disk):
            return df, [], []
        

        X, y = [], []
        image_paths = []
        meta = [] 


        for i, row in tqdm(df.iterrows()):
            labels = np.array([row[l] for l in self.class_labels])
            curr_path = row["local_path"]
            image = io.imread(curr_path) / 255
      
            X.append(np.array([image]))
        

            meta.append(row)
            y.append(labels)
        y = np.array(y)

  
        return meta, X, y
